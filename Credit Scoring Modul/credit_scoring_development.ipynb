{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** \n",
    "<br>\n",
    "Muhammad Insan Aprilian (insanaprilian50@gmail.com)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size:16pt;font-weight:bold\">Problem Statement</font>\n",
    "<br>\n",
    "\n",
    "\n",
    "You are being asked to create a credit scoring model for a lending company. You are given a file with historical data. \n",
    "1.\tThe upper management wants the overall default rate of their portfolio to be below 2.5%, please provide recommendation on the optimal credit score cutoff rate. \n",
    "2.\tPlease create a credit score for each individual, validate your solution, and provide guidance on the next steps. \n",
    "3.\tPlease create deciles by credit score and provide risk and default levels by deciles (by decile and cumulative). Bonus if you can provide confidence (or methodology how you would do it) for your scores/default rates by bin. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16pt;font-weight:bold\">The Data Science Workflow</font>\n",
    "**<p>1.  Import Packages</p>**\n",
    "\n",
    "**<p>2.  Import Data</p>**\n",
    "<p>&nbsp; &nbsp;     2.1.  Metadata Definition</p>\n",
    "\n",
    "**<p>3.  Data Exploration</p>**\n",
    "<p>&nbsp; &nbsp;     3.1.  Missing Value Check</p>\n",
    "<p>&nbsp; &nbsp;     3.2.  Outlier Check (IQR based)</p>\n",
    "<p>&nbsp; &nbsp;     3.3.  Predictor Distribution to Target</p>\n",
    "<p>&nbsp; &nbsp;     3.4.  Data Split</p>\n",
    "<p>&nbsp; &nbsp;     3.5.  Data Transformation</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     3.5.1.  Categorical - Woe Encoder</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     3.5.2.  Numerical - Missing Imputation</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     3.5.3.  Numerical - Standardization</p>\n",
    "\n",
    "**<p>4.  Predictor Selection</p>**    \n",
    "<p>&nbsp; &nbsp;     4.1.  Predictor power comparison</p>\n",
    "<p>&nbsp; &nbsp;     4.2.  Correlations</p>\n",
    "\n",
    "**<p>5.  Modeling</p>**\n",
    "<p>&nbsp; &nbsp;     5.1.  Logistic Regression Session</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.1.1.  Tuning parameter</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.1.2.  Train the model</p>\n",
    "<p>&nbsp; &nbsp;     5.2.  XGBoost Session</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.1.  Train initial model</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.2.  Evaluate predictor</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;     5.2.2.1.  Weight of each predictor</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;     5.2.2.2.  Gain of each predictor</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;     5.2.2.3.  Selected Predictor</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.3.  Tuning parameter</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.4.  Final Model</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.5.  Evaluate final model</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;     5.2.5.1.  Gain of each predictor</p>\n",
    "\n",
    "**<p>6.  Score the dataset</p>**\n",
    "\n",
    "**<p>7.  Performance characteristics</p>**\n",
    "<p>&nbsp; &nbsp;     7.1.  Performance per sample</p>\n",
    "<p>&nbsp; &nbsp;     7.2.  ROC Curve</p>\n",
    "<p>&nbsp; &nbsp;     7.3.  Score Linearity on Holdout Sample</p>\n",
    "<p>&nbsp; &nbsp;     7.4.  Cut-Off Estimation</p>\n",
    "\n",
    "**<p>8.  Conclusion</p>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "\n",
    "- `time` - datetime - ability to get current time for logs\n",
    "- `math` - basic mathematical functions (as logarithm etc.))\n",
    "- `numpy` - for mathematical,and numerical calculations\n",
    "- `scipy` - for metrics evaluation calculations\n",
    "- `pandas` - for work with large data structures\n",
    "- `scikit` - all important machine learning (and statistical) algorithms used for training the models\n",
    "- `matplotlib` - for plotting the charts\n",
    "- `seaborn` - for statistical visualisations\n",
    "- `xgboost` - gradient boosting used for training the models\n",
    "- `category_encoders` - for category type transformation\n",
    "- `ia_pkg` - for combined function used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import ia_pkg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking used library version\n",
    "ia_pkg.pkg_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data from CSV\n",
    "data = pd.read_csv('credit_ds_v3.csv', index_col='X')\n",
    "print('Data loaded on', datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running DataFrame optimizer to reduce memory usage\n",
    "from ia_pkg.function import optimizer\n",
    "data = optimizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with duplicated index\n",
    "data=data[~data.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:',data.shape[0])\n",
    "print('Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = 'default_flag'\n",
    "\n",
    "cols_pred = list(data.drop(col_target,axis=1).columns)\n",
    "\n",
    "cols_pred_num = list(data[cols_pred].select_dtypes(include=np.number).columns)\n",
    "cols_pred_cat = list(data[cols_pred].select_dtypes(include=np.object).columns)\n",
    "\n",
    "print('List of numerical predictors:', len(cols_pred_num),'\\n\\n', data[cols_pred_num].dtypes)\n",
    "print('\\nList of categorical predictors: ', len(cols_pred_cat), '\\n\\n', data[cols_pred_cat].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the statistical summary, could help us on preliminary investigation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate columns with null values\n",
    "missingCol = data.isnull().sum()\n",
    "print(\"There are\", len(missingCol[missingCol != 0]),\"columns with missing value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Investigate null rate of contained null columns\n",
    "missingRate = []\n",
    "for col in cols_pred:\n",
    "    if data[col].isnull().any():\n",
    "        missingRate.append({'Predictor' : col,\n",
    "                       'Missing rate' : data[col].isnull().sum() / data.shape[0]})\n",
    "pd.DataFrame(missingRate).set_index('Predictor').sort_values('Missing rate',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Check (IQR based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ia_pkg.function import cnt_outliers, replace_with_thresholds\n",
    "\n",
    "# Check number of 1.5 IQR based outlier\n",
    "cnt_outliers(data,cols_pred_num,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems all numerical predictor have an outlier, indication that high variability characteristics on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Distribution to Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.plots import stacked_plot, dist_plot\n",
    "stacked_plot(data,\n",
    "            cat_columns=cols_pred_cat,\n",
    "            col_target=col_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, the *'E'* case on the **branch_code** would riskier than the other value, around 2 times riskier (9% on population to 17%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist_plot(data,\n",
    "            columns=cols_pred_num,\n",
    "            col_target=col_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs, explicitely there are several predictors that have very good potential on the model. Their ability to differentiate behavior of default and non-default user is used as the base assumption.\n",
    "\n",
    "Predictor which contained information like **delinquency score**, **Utilization**, **remaining bill** and **overlimit** assumed as the good predictor to the default. But still, further investigation needs to be done (correlation check, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split data into three parts (train,valid,test)\n",
    "- Adds a new column indicating to which part the observation belong\n",
    "- Split is done in random\n",
    "- Set the random seed so the results are replicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.function import data_split\n",
    "data['data_type'] = data_split(data,\n",
    "                               sample_sizes=[0.8,0.1,0.1],\n",
    "                               sample_names=['train','test','valid'],\n",
    "                               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masked the sample name\n",
    "train_mask = (data['data_type'] == 'train')\n",
    "valid_mask = (data['data_type'] == 'valid')\n",
    "test_mask = (data['data_type'] == 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = data.groupby(['data_type']).aggregate({col_target:['sum','count']})\n",
    "data_summary.columns = [col_target, 'rows']\n",
    "data_summary[col_target+' rate'] = data_summary[col_target] / data_summary['rows']\n",
    "\n",
    "display(data_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical - Woe Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WoE method chose to transform the string-type categorical predictor to be in numeric form, WoE estimated the weight of each predictor's unique value for their ability to separate the target(in this case Default/not default).\n",
    "\n",
    "WoE is also flexible with the null value as we can cluster it into 'special segment'. So the imputation would not be needed in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.function import woe_transform\n",
    "#fit and transform WoE on categorical predictor\n",
    "data_woe = woe_transform(data,\n",
    "                         mask=train_mask,\n",
    "                         cat_columns=cols_pred_cat,\n",
    "                         col_target=col_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stored the WoE output on cols_woe\n",
    "data_woe.columns = [i + '_woe' for i in data_woe.columns]\n",
    "cols_woe = list(data_woe.columns)\n",
    "\n",
    "data[cols_woe] = data_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_change = []\n",
    "#Listed the tranformation result on each unique value on categorical predictor\n",
    "for col,col_woe in zip(cols_pred_cat,cols_woe):\n",
    "    woe_change.append(data[[col,col_woe,col_target]].fillna('Null').groupby([col,col_woe]).agg(\n",
    "        {col_woe: ['count'],\n",
    "         col_target : ['sum','mean']}))\n",
    "\n",
    "for i in range(len(woe_change)):\n",
    "    woe_change[i]\n",
    "woe_change[0].columns = [('branch_code_woe count'),\n",
    "            (   'default_flag count'),\n",
    "            (   'default_flag rate')]\n",
    "pd.DataFrame(woe_change[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Missing Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing value imputation is done by filling the mean value to each predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num_missing = data[cols_pred_num].columns[data[cols_pred_num].isnull().any()].tolist()\n",
    "#filling the missing value with mean\n",
    "for c in cols_num_missing:\n",
    "    mean = data[c].mean()\n",
    "    data[c+'_imp'] = data[c].fillna(mean,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is done on numerical predictors to avoid the outlier/bigger magnitude value effects on the model. Standardization is one of the methods for scaling, it transformed all the values by centering its mean at 0 then scales the variance at 1. \n",
    "\n",
    "The pros of this method is it keeping the shape of the predictor's original distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#listed the imputation and non-imputation predictor for scaling\n",
    "cols_pred_num2 = list(map(lambda x: x+'_imp' if x in cols_num_missing else x, cols_pred_num))       \n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(data[train_mask][cols_pred_num2])\n",
    "# print(scaler.mean_)\n",
    "data_sd = scaler.transform(data[train_mask|valid_mask|test_mask][cols_pred_num2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stored the standardscaler output on cols_sd\n",
    "cols_sd = [i+'_sd' for i in cols_pred_num]\n",
    "\n",
    "data[cols_sd] = data_sd\n",
    "data[cols_sd].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapped up all the transformed predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_shortlist = []\n",
    "\n",
    "for c in cols_sd:\n",
    "    cols_shortlist.append(c)\n",
    "for c in cols_woe:\n",
    "    cols_shortlist.append(c)\n",
    "\n",
    "display(cols_shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor Selection\n",
    "<br>\n",
    "\n",
    "Selecting the best predictor for the model, it applied to all transformed predictors. The selection metrics would be **gini, IV** (Predictive power), and **inter-predictor correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive power comparison\n",
    "\n",
    "Calculates IV and Gini of each predictor, sorts the predictors by their power. The power is calculated for each of the samples (train, validate, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.metrics import iv,gini\n",
    "\n",
    "power_tab = []\n",
    "for j in range(0,len(cols_shortlist)):\n",
    "    power_tab.append({'Name':cols_shortlist[j]\n",
    "                    ,'Gini Train':gini(data.loc[train_mask,col_target],data.loc[train_mask,cols_shortlist[j]])                    \n",
    "                    ,'Gini Validate':gini(data.loc[valid_mask,col_target],data.loc[valid_mask,cols_shortlist[j]])\n",
    "                    ,'Gini Test':gini(data.loc[test_mask,col_target],data.loc[test_mask,cols_shortlist[j]])\n",
    "                    ,'IV Train':iv(data.loc[train_mask,col_target],data.loc[train_mask,cols_shortlist[j]])\n",
    "                    ,'IV Validate':iv(data.loc[valid_mask,col_target],data.loc[valid_mask,cols_shortlist[j]])\n",
    "                    ,'IV Test':iv(data.loc[test_mask,col_target],data.loc[test_mask,cols_shortlist[j]])     \n",
    "                     })\n",
    "power_out = pd.DataFrame.from_records(power_tab)\n",
    "power_out = power_out.set_index('Name').abs()\n",
    "power_out = power_out.sort_values('Gini Validate',ascending=False)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "display(power_out)\n",
    "pd.options.display.max_rows = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show correlation matrix of all predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat = data[sorted(cols_shortlist)].corr()\n",
    "\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10), dpi=50)\n",
    "fig.suptitle('Correlations of Variables',fontsize=25)\n",
    "sns.heatmap(cormat, ax=ax, annot=True, fmt=\"0.1f\", linewidths=.5, annot_kws={\"size\":15},cmap=\"OrRd\")\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()\n",
    "plt.clf();plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ok_correlation = 0.5\n",
    "\n",
    "# find highest pairwise correlation (correlation greater than .. in absolute value)\n",
    "hicors = []\n",
    "for i in range(0,len(cormat)):\n",
    "    for j in range(0,len(cormat)):\n",
    "        if ((cormat.iloc[i][j] > max_ok_correlation or cormat.iloc[i][j] < -max_ok_correlation) and i < j):\n",
    "            hicors.append((i,j,cormat.index[i],cormat.index[j],cormat.iloc[i][j],abs(cormat.iloc[i][j])))\n",
    "hicors.sort(key= lambda x: x[5], reverse=True)\n",
    "\n",
    "hicors2 = pd.DataFrame(list(zip(*list(zip(*hicors))[2:5])), columns = ['predictor_1', 'predictor_2', 'corr'])\n",
    "\n",
    "# print list of highest correlations\n",
    "hicors2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining output set from these selection methods, we choosing the predictor which placed on top individual predictive power and eliminate which both ranked on bottom(low gini) and having inter-predictor correlation (>0.5)\n",
    "\n",
    "\n",
    "This new predictor set expected can prevent the low quality and mulitcollinearity issue that may occur on the model(e.g. Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling using two methods (CV Logistic Regression and XGBoost) on training data set. We take a different set of predictors for each model.\n",
    "\n",
    "For Logistic Regression, we take transformed(*WoE* and *Imputation-Standardization*) and selected(*individual gini* and *correlation-based*) predictor called **pred_lr**\n",
    "\n",
    "For XGBoost, we take transformed WoE and non-transformed numerical predictors (leave it as it is), XGBoost decision-tree is robust on outlier and null values so we confidently don't use numerical transformation on this. The set called **pred_xgb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected predictor for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_shortlist2 = ['number_of_cards_sd',\n",
    "#  'outstanding_sd',\n",
    " 'credit_limit_sd',\n",
    "#  'bill_sd',\n",
    "#  'total_cash_usage_sd',\n",
    " 'total_retail_usage_sd',\n",
    "#  'remaining_bill_sd',\n",
    " 'payment_ratio_sd',\n",
    "#  'overlimit_percentage_sd',\n",
    " 'payment_ratio_3month_sd',\n",
    " 'payment_ratio_6month_sd',\n",
    " 'delinquency_score_sd',\n",
    "#  'years_since_card_issuing_sd',\n",
    "#  'total_usage_sd',\n",
    "#  'remaining_bill_per_number_of_cards_sd',\n",
    " 'remaining_bill_per_limit_sd',\n",
    " 'total_usage_per_limit_sd',\n",
    " 'total_3mo_usage_per_limit_sd',\n",
    "#  'total_6mo_usage_per_limit_sd',\n",
    "#  'utilization_3month_sd',\n",
    "#  'utilization_6month_sd',\n",
    " 'branch_code_woe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = cols_shortlist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning is done to know which regularization parameter (C) would be the best to estimate the model, estimated by his ability to balance the bias-variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Grid Search\n",
    "logreg = LogisticRegression(class_weight='balanced',penalty='l2')\n",
    "\n",
    "param = {'C':[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,5000,10000]}\n",
    "gs = GridSearchCV(logreg,param,scoring='roc_auc',refit=True,cv=5)\n",
    "gs.fit(data[train_mask|valid_mask][pred_lr],data[train_mask|valid_mask][col_target])\n",
    "print('Best roc_auc: {:.4}, with best C: {}'.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=gs.best_params_.get('C'))\n",
    "\n",
    "gs = GridSearchCV(logreg,param,refit=True,cv=5)\n",
    "gs.fit(data[train_mask|valid_mask][pred_lr],data[train_mask|valid_mask][col_target])\n",
    "# logreg.fit(data[train_mask|valid_mask][pred_lr],data[train_mask|valid_mask][col_target])\n",
    "\n",
    "lr_scored = gs.predict_proba(data[pred_lr])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted the model's Coefficient and Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = []\n",
    "o.append('|LR MODEL | COEFFICIENTS |\\n| --- | --- |')\n",
    "o.append('| Intercept: | {} |'.format(gs.best_estimator_.intercept_[0]))\n",
    "for p,b in zip(pred_lr,list(gs.best_estimator_.coef_[0])):\n",
    "    o.append('| {} | {} |'.format(p,b))\n",
    "display(Markdown('\\n'.join(o)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient magnitude could tell us the predictor contribution to the model, \n",
    "\n",
    "since we scaling them with the same method, we could say that **delinquency score** *(B=.676)* and **total_usage_per_limit** *(B=-.977)* are the biggest contributorsto the model. **delinquency_score** has a positive and **total_usage_per_limit** has a negative relationship to the defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs xgboost library to be installed.\n",
    "\n",
    "First we train a gradient boosting model using a \"standard\" set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = cols_pred_num + [c + '_woe' for c in cols_pred_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ia_pkg.metrics import gini\n",
    "import xgboost as xgb\n",
    "# pred_xgb.remove('delinquency_score')\n",
    "dt_xgb = data[pred_xgb]\n",
    "\n",
    "xgb_params = {'eta': 0.1,\n",
    "  'max_depth': 3,\n",
    "  'objective': 'binary:logistic',\n",
    "  'eval_metric': 'auc',\n",
    "  'min_child_weight': 30,\n",
    "  'subsample': 0.85}\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "ibooster= xgb.train(params= xgb_params,\n",
    "                        dtrain= xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),\n",
    "                        num_boost_round= 200,\n",
    "                        early_stopping_rounds = 20,\n",
    "                        evals= ((xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),'train'),\n",
    "                                 (xgb.DMatrix(dt_xgb[valid_mask],data[valid_mask][col_target]),'valid')\n",
    "                                ), \n",
    "                        evals_result= evals_result,)\n",
    "\n",
    "ixgb_scored= ibooster.predict(xgb.DMatrix(dt_xgb), ntree_limit=ibooster.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('     Train gini:',gini(data[train_mask][col_target], ixgb_scored[train_mask]))\n",
    "print('Validation gini:',gini(data[valid_mask][col_target], ixgb_scored[valid_mask]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictors evaluated due to their sorted importances on two metrics (weight and gain). At first, we can set the number of predictors which we want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 10 #how many best predictors I want to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight of each predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *n_top* predictors with highest weight (i.e. those which were in most trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb_wgh = [x[0] for x in sorted([(k, v) for k, v in ibooster.get_score(importance_type = 'weight').items()]\\\n",
    "                                     , key=lambda x:x[1], reverse = True)]\n",
    "if len(pred_xgb_wgh) > n_top:\n",
    "    pred_xgb_wgh = pred_xgb_wgh[:n_top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain of each predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *n_top* predictors with highest gain (i.e. relative contribution of the corresponding feature to the model calculated by taking each featureâ€™s contribution for each tree in the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb_gain = [x[0] for x in sorted([(k, v) for k, v in ibooster.get_score(importance_type = 'gain').items()]\\\n",
    "                                      , key=lambda x:x[1], reverse = True)]\n",
    "if len(pred_xgb_gain) > n_top:\n",
    "    pred_xgb_gain = pred_xgb_gain[:n_top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selected Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the final predictors as we combining (union or intersection) the output from each metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(lst1, lst2):\n",
    "    final_list  = list(set(lst1) | set(lst2))\n",
    "    return final_list\n",
    "\n",
    "def intersect(lst1, lst2):\n",
    "    final_list = list(set(lst1) & set(lst2))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = union(pred_xgb_wgh, pred_xgb_gain)\n",
    "display(pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning applied to two inputs (max_depth and learning rate).\n",
    "Tuning set then will be evaluated on the valid sample.\n",
    "\n",
    "There are two options on the best estimation to choose from.\n",
    "\n",
    "*best_valid* for tuning set that has best gini on valid sample\n",
    "\n",
    "*best_diff* for tuning set that has train-valid lowest gini difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.metrics import gini\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dt_xgb = data[pred_xgb]\n",
    "\n",
    "col_result = ['eta', 'max_depth', 'gini_train', 'gini_valid', 'difference']\n",
    "result = pd.DataFrame(columns = col_result)\n",
    "grid_params = {\n",
    "            'eta' : [0.1,0.2,0.3],\n",
    "            'max_depth' : [2,3,4]\n",
    "#               'min_child_weight' : [10,20,30,40,50],\n",
    "#               'subsample' : [0.5, 0.6, 0.7, 0.8, 0.9]      \n",
    "}\n",
    "\n",
    "flag = False\n",
    "\n",
    "for eta in grid_params['eta']:\n",
    "    for max_depth in grid_params['max_depth']:\n",
    "        xgb_params = {'eta': eta,\n",
    "                            'max_depth': max_depth,\n",
    "                            'objective': 'binary:logistic',\n",
    "                            'eval_metric': 'auc',\n",
    "                            'min_child_weight': 30,\n",
    "                            'subsample': 0.85}\n",
    "\n",
    "        evals_result = {}\n",
    "\n",
    "        tbooster = xgb.train(params = xgb_params,\n",
    "                                    dtrain = xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),\n",
    "                                    num_boost_round = 200,\n",
    "                                    early_stopping_rounds = 20,\n",
    "                                    evals = ((xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),'train'),\n",
    "                                             (xgb.DMatrix(dt_xgb[valid_mask],data[valid_mask][col_target]),'valid')\n",
    "                                            ), \n",
    "                                    evals_result = evals_result,)\n",
    "\n",
    "        txgb_scored = tbooster.predict(xgb.DMatrix(dt_xgb), ntree_limit=tbooster.best_ntree_limit)\n",
    "        gini_train = gini(data[train_mask][col_target], txgb_scored[train_mask])\n",
    "        gini_valid = gini(data[valid_mask][col_target], txgb_scored[valid_mask])\n",
    "        added = [eta, max_depth, gini_train, gini_valid, (gini_train-gini_valid)]\n",
    "        if flag == False:\n",
    "            result = pd.DataFrame([added], columns = col_result)\n",
    "            flag = True\n",
    "        else:\n",
    "            result = pd.concat([result, pd.DataFrame([added], columns = col_result)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid = result.loc[result['gini_valid'] == result['gini_valid'].max(),['eta', 'max_depth']].to_dict('list')\n",
    "best_diff = result.loc[result['difference'] == result['difference'].min(),['eta', 'max_depth']].to_dict('list')\n",
    "\n",
    "print('hyperparameter for best_valid: ', best_valid)\n",
    "print('hyperparameter for best_diff, ', best_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dt_xgb = data[pred_xgb]\n",
    "tuning = best_diff # set hyperparameter\n",
    "\n",
    "xgb_params = {'eta': tuning.get('eta')[0],\n",
    "    'max_depth': tuning.get('max_depth')[0],\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'min_child_weight': 30,\n",
    "    'subsample': 0.85}\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "fbooster = xgb.train(params = xgb_params,\n",
    "                        dtrain = xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),\n",
    "                        num_boost_round = 500,\n",
    "                        early_stopping_rounds = 20,\n",
    "                        evals = ((xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),'train'),\n",
    "                                 (xgb.DMatrix(dt_xgb[valid_mask],data[valid_mask][col_target]),'valid')\n",
    "                                ), \n",
    "                        evals_result = evals_result,)\n",
    "\n",
    "fxgb_scored = fbooster.predict(xgb.DMatrix(dt_xgb), ntree_limit=fbooster.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('     Train gini:',gini(data[train_mask][col_target], fxgb_scored[train_mask]))\n",
    "print('Validation gini:',gini(data[valid_mask][col_target], fxgb_scored[valid_mask]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fbooster.get_score(importance_type = 'gain') # available importance types: 'gain', 'cover', 'weight'\n",
    "imp = sorted([(k, v) for k, v in fs.items()], key = lambda x:x[1], reverse = True)\n",
    "imp.reverse()\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.barh(range(len(imp)), [v for k, v in imp], color=\"blue\",  align='center')\n",
    "plt.yticks(range(len(imp)), [k for k, v in imp], fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.xlabel('Importance',fontsize=15)\n",
    "plt.ylim([-1, len(imp)])\n",
    "plt.xlim([0, max([v for k, v in imp])*1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gain importance tells us the predictor relative contribution on each of the tree in the model. As you can see the **total_usage_per_limit**, **total_retail_usage**, and **delinquency_score** are the highest contributor to the model\n",
    "\n",
    "Furthermore, if we want to see the more specific explanation of these predictors SHAP module could be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column with the prediction (probability of default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_score = 'LR_SCORE'\n",
    "col_score1 = 'XGB_SCORE'\n",
    "\n",
    "data[col_score] = lr_scored\n",
    "print('Column',col_score,'with the prediction added/modified. Number of columns:',data.shape[1])\n",
    "\n",
    "data[col_score1] = fxgb_scored\n",
    "print('Column',col_score1,'with the prediction added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance characteristics\n",
    "Performance characteristics of the models (Gini, Lift, KS) and their visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.metrics import gini, lift, kolmogorov_smirnov\n",
    "lift_perc = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = pd.DataFrame({'sample':[\n",
    "    'train',\n",
    "    'valid',\n",
    "    'test'    \n",
    "    ], 'LR_gini':[\n",
    "    gini(data[train_mask][col_target],data[train_mask][col_score]) #train\n",
    "    ,gini(data[valid_mask][col_target],data[valid_mask][col_score]) #valid\n",
    "    ,gini(data[test_mask][col_target],data[test_mask][col_score]) #test\n",
    "    ], 'XGB_gini':[\n",
    "    gini(data[train_mask][col_target],data[train_mask][col_score1]) #train\n",
    "    ,gini(data[valid_mask][col_target],data[valid_mask][col_score1]) #valid\n",
    "    ,gini(data[test_mask][col_target],data[test_mask][col_score1]) #test\n",
    "    ], 'LR_lift'+str(lift_perc):[\n",
    "    lift(data[train_mask][col_target],-data[train_mask][col_score],lift_perc) #train\n",
    "    ,lift(data[valid_mask][col_target],-data[valid_mask][col_score],lift_perc) #valid\n",
    "    ,lift(data[test_mask][col_target],-data[test_mask][col_score],lift_perc) #test\n",
    "    ], 'XGB_lift'+str(lift_perc):[\n",
    "    lift(data[train_mask][col_target],-data[train_mask][col_score1],lift_perc) #train\n",
    "    ,lift(data[valid_mask][col_target],-data[valid_mask][col_score1],lift_perc) #valid\n",
    "    ,lift(data[test_mask][col_target],-data[test_mask][col_score1],lift_perc) #test\n",
    "    ], 'LR_KS':[\n",
    "    kolmogorov_smirnov(data[train_mask][col_score],data[train_mask][col_target]) #train\n",
    "    ,kolmogorov_smirnov(data[valid_mask][col_score],data[valid_mask][col_target]) #valid\n",
    "    ,kolmogorov_smirnov(data[test_mask][col_score],data[test_mask][col_target]) #test\n",
    "    ], 'XGB_KS':[\n",
    "    kolmogorov_smirnov(data[train_mask][col_score1],data[train_mask][col_target]) #train\n",
    "    ,kolmogorov_smirnov(data[valid_mask][col_score1],data[valid_mask][col_target]) #valid\n",
    "    ,kolmogorov_smirnov(data[test_mask][col_score1],data[test_mask][col_target]) #test\n",
    "    ]}).set_index('sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge difference on overall performance generated by these models. Could be data leakage issue on XGB model since the performance on valid and test is too high, need further investigation to prove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute ROC curve for each models\n",
    "fpr = dict()\n",
    "fpr = dict()\n",
    "fpr1 = dict()    \n",
    "tpr1 = dict()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(data[test_mask][col_target], data[test_mask][col_score])\n",
    "fpr1, tpr1, _ = roc_curve(data[test_mask][col_target], data[test_mask][col_score1])\n",
    "\n",
    "#Plot of a ROC curve\n",
    "f, ax1 = plt.subplots(figsize=(6,6))\n",
    "lw = 2\n",
    "ax1.plot(fpr, tpr, color='y',label='LR ROC curve')\n",
    "ax1.plot([0, 1], [0, 1], color='b', lw=lw, linestyle='--') \n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(fpr1, tpr1, color='r',label='XGB ROC curve')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.0])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Receiver operating characteristic')\n",
    "ax1.legend(bbox_to_anchor=(1, 0.1), borderaxespad=0.1)\n",
    "ax2.legend(bbox_to_anchor=(1, 0.05), borderaxespad=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Linearity on Holdout Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.plots import plot_score_linearity\n",
    "plot_score_linearity(data[test_mask],\n",
    "                    col_score=col_score1,\n",
    "                    col_target=col_target,\n",
    "                    bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_linearity(data[test_mask],\n",
    "                    col_score=col_score,\n",
    "                    col_target=col_target,\n",
    "                    bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score distribution is plotted in decile to show the linearity of the output score when we link it to their actual default rate.\n",
    "\n",
    "As you can see XGB could produce more consistent monotonicity than LR model, the reason is higher gini on XGB model\n",
    "\n",
    "Also when we see the PD score on x-axis for both models, it seems the spread tends to gathered at lower PD value, so default threshold 0.5 for cutoff would not be relevant in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut-Off Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.function import cutoff_df\n",
    "from ia_pkg.plots import cutoff_plot\n",
    "\n",
    "# Initialized all variable (expected default rate and scores)\n",
    "exp_def_rate =0.025 # setting up the 2.5% expected default rate\n",
    "scores = [col_score, col_score1]\n",
    "\n",
    "# plot cutoff score\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "for s in scores:\n",
    "    dt = cutoff_df(data[valid_mask|test_mask],\n",
    "                   col_score=s,\n",
    "                   col_target=col_target)\n",
    "    cutoff_plot(dt,\n",
    "                col_score=s,\n",
    "                exp_def_rate=exp_def_rate,\n",
    "                ax=ax)\n",
    "\n",
    "plt.title('Cumulative Distribution - PD Score vs Expected Default Rate')\n",
    "plt.xlabel('PD Score')\n",
    "plt.ylabel('Cummulative default rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answering the problem task, the cutoff score is estimated by plotting the cumulative distribution of default rate to sorted score.\n",
    "\n",
    "Two models presented to see the optimal cutoff to manage below 2.5% default rate on the portfolio\n",
    "\n",
    "From the graph, we can decide for each cutoff score\n",
    "\n",
    "**LR model cutoff score : 0.01482** \n",
    "\n",
    "**XGB model cutoff score : 0.0177**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Credit score is created to predict the probability of default on provided dataset\n",
    "\n",
    "\n",
    "- Transformation and selection procedures is done to find the most useful predictors to the target\n",
    "\n",
    "\n",
    "- Two models(LR and XGB) with different methods are presented to be compared. From the evaluation, XGB has better performance than LR\n",
    "\n",
    "\n",
    "- The *deliquency_score* and *total_usage_per_limit* predictors considered as the highest contributor on both models\n",
    "\n",
    "\n",
    "- Decision for the choosed model not yet to be made since we need further investigation of too high performance and interpretability on one of the model (XGB model)\n",
    "\n",
    "\n",
    "- Optimal cutoff for 2.5% default rate : LR model is 0.01482 and XGB model is 0.0177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "518px",
    "width": "517px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "826px",
    "left": "89px",
    "top": "180px",
    "width": "371.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
