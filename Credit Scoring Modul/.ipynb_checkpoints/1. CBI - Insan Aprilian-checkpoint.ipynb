{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20pt;font-weight:bold\">Credit Scoring Case</font>\n",
    "<br>\n",
    "\n",
    "**Author:** \n",
    "<br>\n",
    "Muhammad Insan Aprilian (insanaprilian50@gmail.com)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size:16pt;font-weight:bold\">Problem Statement</font>\n",
    "<br>\n",
    "\n",
    "\n",
    "Build a machine learning model to predict if an applicant is 'good' or 'bad' client. You should define the 'good' or 'bad' by yourself by using vintage analysis or any other methods. Be creative in constructing the analysis and provide recommendations to the business based on the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16pt;font-weight:bold\">The Data Science Workflow</font>\n",
    "**<p>1.  Import Packages</p>**\n",
    "\n",
    "**<p>2.  Import Data</p>**\n",
    "<p>&nbsp; &nbsp;     2.1.  Metadata Definition</p>\n",
    "\n",
    "**<p>3.  Data Exploration</p>**\n",
    "<p>&nbsp; &nbsp;     3.1.  Missing Value Check</p>\n",
    "<p>&nbsp; &nbsp;     3.2.  Outlier Check (IQR based)</p>\n",
    "<p>&nbsp; &nbsp;     3.3.  Predictor Distribution to Target</p>\n",
    "<p>&nbsp; &nbsp;     3.4.  Data Split</p>\n",
    "<p>&nbsp; &nbsp;     3.5.  Data Transformation</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     3.5.1.  Categorical - Woe Encoder</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     3.5.2.  Numerical - Missing Imputation</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     3.5.3.  Numerical - Standardization</p>\n",
    "\n",
    "**<p>4.  Predictor Selection</p>**    \n",
    "<p>&nbsp; &nbsp;     4.1.  Predictor power comparison</p>\n",
    "<p>&nbsp; &nbsp;     4.2.  Correlations</p>\n",
    "\n",
    "**<p>5.  Modeling</p>**\n",
    "<p>&nbsp; &nbsp;     5.1.  Logistic Regression Session</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.1.1.  Tuning parameter</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.1.2.  Train the model</p>\n",
    "<p>&nbsp; &nbsp;     5.2.  XGBoost Session</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.1.  Train initial model</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.2.  Evaluate predictor</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;     5.2.2.1.  Weight of each predictor</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;     5.2.2.2.  Gain of each predictor</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;     5.2.2.3.  Selected Predictor</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.3.  Tuning parameter</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.4.  Final Model</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp;     5.2.5.  Evaluate final model</p>\n",
    "<p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;     5.2.5.1.  Gain of each predictor</p>\n",
    "\n",
    "**<p>6.  Score the dataset</p>**\n",
    "\n",
    "**<p>7.  Models performance evaluation</p>**\n",
    "<p>&nbsp; &nbsp;     7.1.  Performance per sample</p>\n",
    "<p>&nbsp; &nbsp;     7.2.  ROC Curve</p>\n",
    "<p>&nbsp; &nbsp;     7.3.  Score Linearity on Holdout Sample</p>\n",
    "<p>&nbsp; &nbsp;     7.4.  Cut-Off Estimation</p>\n",
    "\n",
    "**<p>8.  Defined target comparison</p>**\n",
    "\n",
    "**<p>9.  Threshold Evaluation</p>**\n",
    "<p>&nbsp; &nbsp;     9.1.  Evaluate default threshold (p=0.5) to different metric</p>\n",
    "<p>&nbsp; &nbsp;     9.2.  Threshold Adjustment</p>\n",
    "<p>&nbsp; &nbsp;     9.3.  Evaluate selected threshold to different metric</p>\n",
    "<p>&nbsp; &nbsp;     9.4.  Cut-Off Estimation</p>\n",
    "\n",
    "**<p>8.  Conclusion</p>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "\n",
    "- `time` - datetime - ability to get current time for logs\n",
    "- `math` - basic mathematical functions (as logarithm etc.))\n",
    "- `numpy` - for mathematical,and numerical calculations\n",
    "- `scipy` - for metrics evaluation calculations\n",
    "- `pandas` - for work with large data structures\n",
    "- `scikit` - all important machine learning (and statistical) algorithms used for training the models\n",
    "- `matplotlib` - for plotting the charts\n",
    "- `seaborn` - for statistical visualisations\n",
    "- `xgboost` - gradient boosting used for training the models\n",
    "- `category_encoders` - for category type transformation\n",
    "- `ia_pkg` - for combined function used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import ia_pkg\n",
    "# import ia_pkg_old\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "from numpy import arange\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from geopy.geocoders import Nominatim\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking used library version\n",
    "ia_pkg.pkg_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data from CSV\n",
    "data = pd.read_csv('sample_200_0k_20170120.csv')\n",
    "data.columns = [i.lower() for i in data.columns] \n",
    "print('Data loaded on', datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running DataFrame optimizer to reduce memory usage\n",
    "from ia_pkg.function import optimizer\n",
    "data = optimizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with duplicated index\n",
    "data=data[~data.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:',data.shape[0])\n",
    "print('Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['defaulted'] = np.where(data['maxoverduedays']>90,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat2 = math.radians(lat2)\n",
    "    lon2 = math.radians(lon2)\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "data['birthdate'] = pd.to_datetime(data['birthdate'])\n",
    "data['newapplicationdate'] = pd.to_datetime(data['newapplicationdate'])\n",
    "data['previous_clean'] = data['previous'].str.split(',').str[0]\n",
    "data['previous_clean'] = pd.to_datetime(data['previous_clean'])\n",
    "data['homestatus'] = data['homestatus'].astype(str)\n",
    "\n",
    "# Apply the function to categorize the data\n",
    "data['distance_residence_legal'] = data.apply(lambda row: haversine(row['residence_lat'], row['residence_long'], row['legal_lat'], row['legal_long']), axis=1)\n",
    "data['distance_residence_company'] = data.apply(lambda row: haversine(row['residence_lat'], row['residence_long'], row['company_lat'], row['company_long']), axis=1)\n",
    "data['distance_legal_company'] = data.apply(lambda row: haversine(row['legal_lat'], row['legal_long'], row['company_lat'], row['company_long']), axis=1)\n",
    "\n",
    "data['newapplicationmonth'] = np.floor((datetime.now(timezone.utc) - data['newapplicationdate']).dt.days/30)\n",
    "data['previousmonth'] = np.floor((datetime.now() - data['previous_clean']).dt.days/30)\n",
    "data['age'] = np.floor((datetime.now(timezone.utc) - data['birthdate']).dt.days/365)\n",
    "data['stayyear'] = 2023-data['staysinceyear']\n",
    "data['employmentyear'] = 2023-data['employmentsinceyear']\n",
    "data['mainbusinessyear'] = 2023-data['mainbusinesssinceyear']\n",
    "\n",
    "data['samehomelegaladdress'] = np.where(data['residencezipcode']==data['legalzipcode'],1,0)\n",
    "data['samehomecompanyaddress'] = np.where(data['residencezipcode']==data['companyzipcode'],1,0)\n",
    "data['samelegalcompanyaddress'] = np.where(data['legalzipcode']==data['companyzipcode'],1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = 'defaulted'\n",
    "# data[col_target] = data[col_target].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "cols_pred = [\n",
    "        'gender',  'maritalstatus', 'numofdependence', 'education',\n",
    "       'professionid', 'homestatus', 'jobtypeid', 'jobpos', 'monthlyfixedincome',\n",
    "       'monthlyvariableincome', 'spouseincome',\n",
    "       'birthplace', 'avg_income', 'std_income',\n",
    "       'avg_income_cnt', 'avg_income_nation', 'std_income_nation',\n",
    "       'avg_income_nation_cnt', 'avg_income_area', 'std_icnome_area',\n",
    "       'avg_income_area_cnt', 'avg_sale_house_price_5000',\n",
    "       'std_sale_house_price_5000', 'sale_house_cnt_5000',\n",
    "       'avg_sale_apartment_price_5000', 'std_sale_apartment_price_5000',\n",
    "       'sale_apartment_cnt_5000', 'avg_rent_house_price_5000',\n",
    "       'std_rent_house_price_5000', 'rent_house_cnt_5000',\n",
    "       'avg_rent_apartment_price_5000', 'std_rent_apartment_price_5000',\n",
    "       'rent_apartment_cnt_5000', 'avg_sale_house_price_10000',\n",
    "       'std_sale_house_price_10000', 'sale_house_cnt_10000',\n",
    "       'avg_sale_apartment_price_10000', 'std_sale_apartment_price_10000',\n",
    "       'sale_apartment_cnt_10000', 'avg_rent_house_price_10000',\n",
    "       'std_rent_house_price_10000', 'rent_house_cnt_10000',\n",
    "       'avg_rent_apartment_price_10000', 'std_rent_apartment_price_10000',\n",
    "       'rent_apartment_cnt_10000', 'distance_residence_company',\n",
    "       'previous_clean', 'newapplicationmonth', 'previousmonth',\n",
    "       'age', 'stayyear', 'employmentyear', 'mainbusinessyear',\n",
    "       'samehomelegaladdress', 'samehomecompanyaddress',\n",
    "       'samelegalcompanyaddress','distance_residence_legal','distance_legal_company'\n",
    "    ]\n",
    "\n",
    "cols_pred_num = list(data[cols_pred].select_dtypes(include=np.number).columns)\n",
    "cols_pred_cat = list(data[cols_pred].select_dtypes(include=np.object).columns)\n",
    "\n",
    "print('List of numerical predictors:', len(cols_pred_num),'\\n\\n', data[cols_pred_num].dtypes)\n",
    "print('\\nList of categorical predictors: ', len(cols_pred_cat), '\\n\\n', data[cols_pred_cat].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the statistical summary, could help us on preliminary investigation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "data.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate columns with null values\n",
    "missingCol = data.isnull().sum()\n",
    "print(\"There are\", len(missingCol[missingCol != 0]),\"columns with missing value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Investigate null rate of contained null columns\n",
    "missingRate = []\n",
    "for col in cols_pred:\n",
    "    if data[col].isnull().any():\n",
    "        missingRate.append({'Predictor' : col,\n",
    "                       'Missing rate' : data[col].isnull().sum() / data.shape[0]})\n",
    "pd.DataFrame(missingRate).set_index('Predictor').sort_values('Missing rate',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Check (IQR based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ia_pkg.function import cnt_outliers, replace_with_thresholds\n",
    "\n",
    "# Check number of 1.5 IQR based outlier\n",
    "cnt_outliers(data,cols_pred_num,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems all numerical predictor have an outlier, indication that high variability characteristics on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Distribution to Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from ia_pkg.plots import stacked_plot, dist_plot\n",
    "# stacked_plot(data,\n",
    "#             cat_columns=cols_pred_cat,\n",
    "#             col_target=col_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, the *'IT staff'* case on the **occupation_type** would riskier than the other value, around 2 times riskier (9% on population to 17%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Predictor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dist_plot(data,\n",
    "#             columns=cols_pred_num,\n",
    "#             col_target=col_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs, explicitely there are several predictors that have very good potential on the model. Their ability to differentiate behavior of bad and good customer is used as the base assumption.\n",
    "\n",
    "Predictor which contained information like **age**, and **months_employed** assumed as the good predictor to the bad. But still, further investigation needs to be done (correlation check, etc)\n",
    "\n",
    "On the side note, **months_unemployed** show a strange pattern where the value concentrated only on one value, it could be an input error case, so then we simplified it with a new predictor **flag_unemployed** that only state YES/NO for being unemployed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split data into three parts (train,valid,test)\n",
    "- Adds a new column indicating to which part the observation belong\n",
    "- Split is done in random\n",
    "- Set the random seed so the results are replicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.function import data_split\n",
    "data['data_type'] = data_split(data,\n",
    "                               sizes=[0.6,0.2,0.2],\n",
    "                               names=['train','test','valid'],\n",
    "                               seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masked the sample name\n",
    "train_mask = (data['data_type'] == 'train')\n",
    "valid_mask = (data['data_type'] == 'valid')\n",
    "test_mask = (data['data_type'] == 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = data.groupby(['data_type']).aggregate({col_target:['sum','count']})\n",
    "data_summary.columns = [col_target, 'rows']\n",
    "data_summary[col_target+' rate'] = data_summary[col_target] / data_summary['rows']\n",
    "\n",
    "display(data_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical - Woe Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WoE method chose to transform the string-type categorical predictor to be in numeric form, WoE estimated the weight of each predictor's unique value for their ability to separate the target(in this case Default/not default).\n",
    "\n",
    "WoE is also flexible with the null value as we can cluster it into 'special segment'. So the imputation would not be needed in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.function import woe_transform\n",
    "#fit and transform WoE on categorical predictor\n",
    "data_woe = woe_transform(data,\n",
    "                         mask=train_mask,\n",
    "                         cat_columns=cols_pred_cat,\n",
    "                         col_target=col_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stored the WoE output on cols_woe\n",
    "data_woe.columns = [i + '_woe' for i in data_woe.columns]\n",
    "cols_woe = list(data_woe.columns)\n",
    "\n",
    "data[cols_woe] = data_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_change = []\n",
    "#Listed the tranformation result on each unique value on categorical predictor\n",
    "for col,col_woe in zip(cols_pred_cat,cols_woe):\n",
    "    woe_change.append(data[[col,col_woe,col_target]].fillna('Null').groupby([col,col_woe]).agg(\n",
    "        {col_woe: ['count'],\n",
    "         col_target : ['sum','mean']}))\n",
    "\n",
    "for i in range(len(woe_change)):\n",
    "    woe_change[i]\n",
    "woe_change[0].columns = [('branch_code_woe count'),\n",
    "            (   'default_flag count'),\n",
    "            (   'default_flag rate')]\n",
    "pd.DataFrame(woe_change[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Missing Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing value imputation is done by filling the mean value to each predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num_missing = data[cols_pred_num].columns[data[cols_pred_num].isnull().any()].tolist()\n",
    "#filling the missing value with mean\n",
    "for c in cols_num_missing:\n",
    "    mean = data[c].mean()\n",
    "    data[c+'_imp'] = data[c].fillna(mean,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical - Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is done on numerical predictors to avoid the outlier/bigger magnitude value effects on the model. Standardization is one of the methods for scaling, it transformed all the values by centering its mean at 0 then scales the variance at 1. \n",
    "\n",
    "The pros of this method is it keeping the shape of the predictor's original distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#listed the imputation and non-imputation predictor for scaling\n",
    "cols_pred_num2 = list(map(lambda x: x+'_imp' if x in cols_num_missing else x, cols_pred_num))       \n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(data[train_mask][cols_pred_num2])\n",
    "# print(scaler.mean_)\n",
    "data_sd = scaler.transform(data[train_mask|valid_mask|test_mask][cols_pred_num2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stored the standardscaler output on cols_sd\n",
    "cols_sd = [i+'_sd' for i in cols_pred_num]\n",
    "\n",
    "data[cols_sd] = data_sd\n",
    "data[cols_sd].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapped up all the transformed predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_shortlist = []\n",
    "\n",
    "for c in cols_sd:\n",
    "    cols_shortlist.append(c)\n",
    "for c in cols_woe:\n",
    "    cols_shortlist.append(c)\n",
    "\n",
    "display(cols_shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor Selection\n",
    "<br>\n",
    "\n",
    "Selecting the best predictor for the model, it applied to all transformed predictors. The selection metrics would be **gini, IV** (Predictive power), and **inter-predictor correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive power comparison\n",
    "\n",
    "Calculates IV and Gini of each predictor, sorts the predictors by their power. The power is calculated for each of the samples (train, validate, test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.metrics import iv,gini\n",
    "\n",
    "# power_tab = []\n",
    "# for j in range(0,len(cols_shortlist)):\n",
    "#     power_tab.append({'Name':cols_shortlist[j]\n",
    "#                     ,'Gini Train':gini(data.loc[train_mask,col_target],data.loc[train_mask,cols_shortlist[j]])                    \n",
    "#                     ,'Gini Validate':gini(data.loc[valid_mask,col_target],data.loc[valid_mask,cols_shortlist[j]])\n",
    "# #                     ,'Gini Test':gini(data.loc[test_mask,col_target],data.loc[test_mask,cols_shortlist[j]])\n",
    "#                     ,'IV Train':iv(data.loc[train_mask,col_target],data.loc[train_mask,cols_shortlist[j]])\n",
    "#                     ,'IV Validate':iv(data.loc[valid_mask,col_target],data.loc[valid_mask,cols_shortlist[j]])\n",
    "# #                     ,'IV Test':iv(data.loc[test_mask,col_target],data.loc[test_mask,cols_shortlist[j]])     \n",
    "#                      })\n",
    "# power_out = pd.DataFrame.from_records(power_tab)\n",
    "# power_out = power_out.set_index('Name').abs()\n",
    "power_out = power_out.sort_values('Gini Train',ascending=False)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "display(power_out)\n",
    "pd.options.display.max_rows = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show correlation matrix of all predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat = data[sorted(cols_shortlist)].corr()\n",
    "\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10), dpi=50)\n",
    "fig.suptitle('Correlations of Variables',fontsize=25)\n",
    "sns.heatmap(cormat, ax=ax, annot=True, fmt=\"0.1f\", linewidths=.5, annot_kws={\"size\":15},cmap=\"OrRd\")\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()\n",
    "plt.clf();plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the unemployed case occur on the older age customer, which is aligned with the retirement period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ok_correlation = 0.5\n",
    "\n",
    "# find highest pairwise correlation (correlation greater than .. in absolute value)\n",
    "hicors = []\n",
    "for i in range(0,len(cormat)):\n",
    "    for j in range(0,len(cormat)):\n",
    "        if ((cormat.iloc[i][j] > max_ok_correlation or cormat.iloc[i][j] < -max_ok_correlation) and i < j):\n",
    "            hicors.append((i,j,cormat.index[i],cormat.index[j],cormat.iloc[i][j],abs(cormat.iloc[i][j])))\n",
    "hicors.sort(key= lambda x: x[5], reverse=True)\n",
    "\n",
    "hicors2 = pd.DataFrame(list(zip(*list(zip(*hicors))[2:5])), columns = ['predictor_1', 'predictor_2', 'corr'])\n",
    "\n",
    "# print list of highest correlations\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "display(hicors2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining output set from these selection methods, we choosing the predictor which placed on top individual predictive power and eliminate which both ranked on bottom(low gini) and having inter-predictor correlation (>0.5)\n",
    "\n",
    "\n",
    "This new predictor set expected can prevent the low quality and mulitcollinearity issue that may occur on the model(e.g. Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling using two methods (CV Logistic Regression and XGBoost) on training data set. We take a different set of predictors for each model.\n",
    "\n",
    "For Logistic Regression, we take transformed(*WoE* and *Imputation-Standardization*) and selected(*individual gini* and *correlation-based*) predictor called **pred_lr**\n",
    "\n",
    "For XGBoost, we take transformed WoE and non-transformed numerical predictors (leave it as it is), XGBoost decision-tree is robust on outlier and null values so we confidently don't use numerical transformation on this. The set called **pred_xgb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected predictor for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove for invariant case: flag_mobil\n",
    "## remove for error input case: months_unemployed\n",
    "## remove for correlation case:  flag_unemployed\n",
    "## remove for poor gini case: flag_phone,cnt_children,cnt_fam_members, flag_work_phone, name_income_type,\n",
    "#        months_employed, flag_email, amt_income_total\n",
    "\n",
    "pred_lr = [\n",
    "    'birthplace_woe','jobpos_woe','monthlyfixedincome_sd','professionid_woe','education_woe',\n",
    "    'avg_income_area_cnt_sd','samehomecompanyaddress_sd','std_rent_apartment_price_5000_sd',\n",
    "    'std_sale_apartment_price_5000_sd','newapplicationmonth_sd','mainbusinessyear_sd','age_sd','homestatus_woe',\n",
    "    'rent_house_cnt_10000_sd'\n",
    "    \n",
    "          ] \n",
    "len(pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning is done to know which regularization parameter (C) would be the best to estimate the model, estimated by his ability to balance the bias-variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Grid Search\n",
    "logreg = LogisticRegression(class_weight='balanced',penalty='l2')\n",
    "\n",
    "param = {'C':[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,5000,10000]}\n",
    "gs = GridSearchCV(logreg,param,scoring='roc_auc',refit=True,cv=5)\n",
    "gs.fit(data[train_mask][pred_lr],data[train_mask][col_target])\n",
    "print('Best roc_auc: {:.4}, with best C: {}'.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logregCV = LogisticRegressionCV(Cs=param.get('C'),cv=5)\n",
    "lr = logregCV.fit(data[train_mask|valid_mask][pred_lr],\n",
    "                  data[train_mask|valid_mask][col_target])\n",
    "\n",
    "lr_scored = lr.predict_proba(data[pred_lr])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted the model's Coefficient and Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = []\n",
    "o.append('|LR MODEL | COEFFICIENTS |\\n| --- | --- |')\n",
    "o.append('| Intercept: | {} |'.format(lr.intercept_[0]))\n",
    "for p,b in zip(pred_lr,list(lr.coef_[0])):\n",
    "    o.append('| {} | {} |'.format(p,b))\n",
    "display(Markdown('\\n'.join(o)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient magnitude could tell us the predictor contribution to the model, \n",
    "\n",
    "since we scaling them with the same method, we could say that **age** *(B=**-.072**)* and **occupation_type** *(B=**.013**)* are the biggest contributors to the model. Where **age** has a negative and **occupation_type** has a positive relationship to the bad client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs xgboost library to be installed.\n",
    "\n",
    "First we train a gradient boosting model using a \"standard\" set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = cols_pred_num + cols_woe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ia_pkg.metrics import gini\n",
    "import xgboost as xgb\n",
    "# pred_xgb.remove('delinquency_score')\n",
    "dt_xgb = data[pred_xgb]\n",
    "\n",
    "xgb_params = {'eta': 0.1,\n",
    "  'max_depth': 3,\n",
    "  'objective': 'binary:logistic',\n",
    "  'eval_metric': 'auc',\n",
    "  'min_child_weight': 30,\n",
    "  'subsample': 0.85}\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "ibooster= xgb.train(params= xgb_params,\n",
    "                        dtrain= xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),\n",
    "                        num_boost_round= 200,\n",
    "                        early_stopping_rounds = 20,\n",
    "                        evals= ((xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),'train'),\n",
    "                                 (xgb.DMatrix(dt_xgb[valid_mask],data[valid_mask][col_target]),'valid')\n",
    "                                ), \n",
    "                        evals_result= evals_result,)\n",
    "\n",
    "ixgb_scored= ibooster.predict(xgb.DMatrix(dt_xgb), ntree_limit=ibooster.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('     Train gini:',gini(data[train_mask][col_target], ixgb_scored[train_mask]))\n",
    "print('Validation gini:',gini(data[valid_mask][col_target], ixgb_scored[valid_mask]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictors evaluated due to their sorted importances on two metrics (weight and gain). At first, we can set the number of predictors which we want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 9 #how many best predictors I want to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight of each predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *n_top* predictors with highest weight (i.e. those which were in most trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb_wgh = [x[0] for x in sorted([(k, v) for k, v in ibooster.get_score(importance_type = 'weight').items()]\\\n",
    "                                     , key=lambda x:x[1], reverse = True)]\n",
    "if len(pred_xgb_wgh) > n_top:\n",
    "    pred_xgb_wgh = pred_xgb_wgh[:n_top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain of each predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *n_top* predictors with highest gain (i.e. relative contribution of the corresponding feature to the model calculated by taking each featureâ€™s contribution for each tree in the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb_gain = [x[0] for x in sorted([(k, v) for k, v in ibooster.get_score(importance_type = 'gain').items()]\\\n",
    "                                      , key=lambda x:x[1], reverse = True)]\n",
    "if len(pred_xgb_gain) > n_top:\n",
    "    pred_xgb_gain = pred_xgb_gain[:n_top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selected Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the final predictors as we combining (union or intersection) the output from each metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(lst1, lst2):\n",
    "    final_list  = list(set(lst1) | set(lst2))\n",
    "    return final_list\n",
    "\n",
    "def intersect(lst1, lst2):\n",
    "    final_list = list(set(lst1) & set(lst2))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = union(pred_xgb_wgh, pred_xgb_gain)\n",
    "display(pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning applied to two inputs (max_depth and learning rate).\n",
    "Tuning set then will be evaluated on the valid sample.\n",
    "\n",
    "There are two options on the best estimation to choose from.\n",
    "\n",
    "*best_valid* for tuning set that has best gini on valid sample\n",
    "\n",
    "*best_diff* for tuning set that has train-valid lowest gini difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.metrics import gini\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dt_xgb = data[pred_xgb]\n",
    "\n",
    "col_result = ['eta', 'max_depth', 'gini_train', 'gini_valid', 'difference']\n",
    "result = pd.DataFrame(columns = col_result)\n",
    "grid_params = {\n",
    "            'eta' : [0.1,0.2,0.3],\n",
    "            'max_depth' : [2,3,4]\n",
    "#               'min_child_weight' : [10,20,30,40,50],\n",
    "#               'subsample' : [0.5, 0.6, 0.7, 0.8, 0.9]      \n",
    "}\n",
    "\n",
    "flag = False\n",
    "\n",
    "for eta in grid_params['eta']:\n",
    "    for max_depth in grid_params['max_depth']:\n",
    "        xgb_params = {'eta': eta,\n",
    "                            'max_depth': max_depth,\n",
    "                            'objective': 'binary:logistic',\n",
    "                            'eval_metric': 'auc',\n",
    "                            'min_child_weight': 30,\n",
    "                            'subsample': 0.85}\n",
    "\n",
    "        evals_result = {}\n",
    "\n",
    "        tbooster = xgb.train(params = xgb_params,\n",
    "                                    dtrain = xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),\n",
    "                                    num_boost_round = 200,\n",
    "                                    early_stopping_rounds = 20,\n",
    "                                    evals = ((xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),'train'),\n",
    "                                             (xgb.DMatrix(dt_xgb[valid_mask],data[valid_mask][col_target]),'valid')\n",
    "                                            ), \n",
    "                                    evals_result = evals_result,)\n",
    "\n",
    "        txgb_scored = tbooster.predict(xgb.DMatrix(dt_xgb), ntree_limit=tbooster.best_ntree_limit)\n",
    "        gini_train = gini(data[train_mask][col_target], txgb_scored[train_mask])\n",
    "        gini_valid = gini(data[valid_mask][col_target], txgb_scored[valid_mask])\n",
    "        added = [eta, max_depth, gini_train, gini_valid, (gini_train-gini_valid)]\n",
    "        if flag == False:\n",
    "            result = pd.DataFrame([added], columns = col_result)\n",
    "            flag = True\n",
    "        else:\n",
    "            result = pd.concat([result, pd.DataFrame([added], columns = col_result)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid = result.loc[result['gini_valid'] == result['gini_valid'].max(),['eta', 'max_depth']].to_dict('list')\n",
    "best_diff = result.loc[result['difference'] == result['difference'].min(),['eta', 'max_depth']].to_dict('list')\n",
    "\n",
    "print('hyperparameter for best_valid: ', best_valid)\n",
    "print('hyperparameter for best_diff, ', best_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dt_xgb = data[pred_xgb]\n",
    "tuning = best_diff # set hyperparameter\n",
    "\n",
    "xgb_params = {'eta': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'min_child_weight': 30,\n",
    "    'subsample': 0.85}\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "fbooster = xgb.train(params = xgb_params,\n",
    "                        dtrain = xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),\n",
    "                        num_boost_round = 500,\n",
    "                        early_stopping_rounds = 20,\n",
    "                        evals = ((xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),'train'),\n",
    "                                 (xgb.DMatrix(dt_xgb[valid_mask],data[valid_mask][col_target]),'valid')\n",
    "                                ), \n",
    "                        evals_result = evals_result,)\n",
    "\n",
    "fxgb_scored = fbooster.predict(xgb.DMatrix(dt_xgb), ntree_limit=fbooster.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('     Train gini:',gini(data[train_mask][col_target], fxgb_scored[train_mask]))\n",
    "print('Validation gini:',gini(data[valid_mask][col_target], fxgb_scored[valid_mask]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fbooster.get_score(importance_type = 'gain') # available importance types: 'gain', 'cover', 'weight'\n",
    "imp = sorted([(k, v) for k, v in fs.items()], key = lambda x:x[1], reverse = True)\n",
    "imp.reverse()\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.barh(range(len(imp)), [v for k, v in imp], color=\"blue\",  align='center')\n",
    "plt.yticks(range(len(imp)), [k for k, v in imp], fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.xlabel('Importance',fontsize=15)\n",
    "plt.ylim([-1, len(imp)])\n",
    "plt.xlim([0, max([v for k, v in imp])*1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gain importance tells us the predictor relative contribution on each of the tree in the model. \n",
    "\n",
    "As you can see the **flag_own_realty**, **name_family_status**, and **name_education_type** are the highest contributor to the model\n",
    "\n",
    "Furthermore, if we want to see the more specific explanation of these predictors SHAP module could be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column with the prediction (probability of default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_score = 'LR_SCORE'\n",
    "col_score1 = 'XGB_SCORE'\n",
    "\n",
    "data[col_score] = lr_scored\n",
    "print('Column',col_score,'with the prediction added/modified. Number of columns:',data.shape[1])\n",
    "\n",
    "data[col_score1] = fxgb_scored\n",
    "print('Column',col_score1,'with the prediction added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models performance evaluation\n",
    "Performance characteristics of the models (Gini, Lift, KS) and their visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.metrics import gini, lift, kolmogorov_smirnov\n",
    "lift_perc = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = pd.DataFrame({'sample':[\n",
    "    'all',\n",
    "    'train',\n",
    "    'valid',\n",
    "    'test'    \n",
    "    ], 'LR_gini':[\n",
    "    gini(data[col_target],data[col_score]) #train\n",
    "    ,gini(data[train_mask][col_target],data[train_mask][col_score]) #train\n",
    "    ,gini(data[valid_mask][col_target],data[valid_mask][col_score]) #valid\n",
    "    ,gini(data[test_mask][col_target],data[test_mask][col_score]) #test\n",
    "    ], 'XGB_gini':[\n",
    "    gini(data[col_target],data[col_score1]) #train\n",
    "    ,gini(data[train_mask][col_target],data[train_mask][col_score1]) #train\n",
    "    ,gini(data[valid_mask][col_target],data[valid_mask][col_score1]) #valid\n",
    "    ,gini(data[test_mask][col_target],data[test_mask][col_score1]) #test\n",
    "    ], 'LR_lift'+str(lift_perc):[\n",
    "    lift(data[col_target],-data[col_score],lift_perc) #train\n",
    "    ,lift(data[train_mask][col_target],-data[train_mask][col_score],lift_perc) #train\n",
    "    ,lift(data[valid_mask][col_target],-data[valid_mask][col_score],lift_perc) #valid\n",
    "    ,lift(data[test_mask][col_target],-data[test_mask][col_score],lift_perc) #test\n",
    "    ], 'XGB_lift'+str(lift_perc):[\n",
    "    lift(data[col_target],-data[col_score1],lift_perc) #train\n",
    "    ,lift(data[train_mask][col_target],-data[train_mask][col_score1],lift_perc) #train\n",
    "    ,lift(data[valid_mask][col_target],-data[valid_mask][col_score1],lift_perc) #valid\n",
    "    ,lift(data[test_mask][col_target],-data[test_mask][col_score1],lift_perc) #test\n",
    "    ], 'LR_KS':[\n",
    "    kolmogorov_smirnov(data[col_score],data[col_target]) #train\n",
    "    ,kolmogorov_smirnov(data[train_mask][col_score],data[train_mask][col_target]) #train\n",
    "    ,kolmogorov_smirnov(data[valid_mask][col_score],data[valid_mask][col_target]) #valid\n",
    "    ,kolmogorov_smirnov(data[test_mask][col_score],data[test_mask][col_target]) #test\n",
    "    ], 'XGB_KS':[\n",
    "    kolmogorov_smirnov(data[col_score1],data[col_target]) #train\n",
    "    ,kolmogorov_smirnov(data[train_mask][col_score1],data[train_mask][col_target]) #train\n",
    "    ,kolmogorov_smirnov(data[valid_mask][col_score1],data[valid_mask][col_target]) #valid\n",
    "    ,kolmogorov_smirnov(data[test_mask][col_score1],data[test_mask][col_target]) #test\n",
    "    ]}).set_index('sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge difference on overall performance generated by these models. XGB produce higher quality model on all sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute ROC curve for each models\n",
    "fpr, tpr, _ = roc_curve(data[test_mask][col_target], data[test_mask][col_score])\n",
    "fpr1, tpr1, _ = roc_curve(data[test_mask][col_target], data[test_mask][col_score1])\n",
    "\n",
    "#Plot of a ROC curve\n",
    "f, ax1 = plt.subplots(figsize=(6,6))\n",
    "lw = 2\n",
    "ax1.plot(fpr, tpr, color='y',label='LR ROC curve')\n",
    "ax1.plot([0, 1], [0, 1], color='b', lw=lw, linestyle='--') \n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(fpr1, tpr1, color='r',label='XGB ROC curve')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.0])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Receiver operating characteristic')\n",
    "ax1.legend(bbox_to_anchor=(1, 0.1), borderaxespad=0.1)\n",
    "ax2.legend(bbox_to_anchor=(1, 0.05), borderaxespad=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Linearity on Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ia_pkg.plots import plot_score_linearity\n",
    "plot_score_linearity(data[test_mask],\n",
    "                    col_score=col_score,\n",
    "                    col_target=col_target,\n",
    "                    bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_linearity(data[test_mask],\n",
    "                    col_score=col_score1,\n",
    "                    col_target=col_target,\n",
    "                    bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score distribution is plotted in decile to show the linearity of the output score when we link it to their actual bad rate.\n",
    "\n",
    "As you can see XGB could produce more consistent monotonicity than LR model, the reason is higher Gini on XGB model. \n",
    "\n",
    "**XGB model then selected as the final Model**\n",
    "\n",
    "Also when we see the PB score on x-axis for both models, it seems the spread also tends to gathered at lower PB value, so default threshold 0.5 for cutoff would not be relevant in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate default threshold (p=0.5) to different metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove last statement, we will simulated the impact of using default threshold on imbalanced dataset, the different evaluation metrics presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([accuracy_score(data[test_mask][col_target],np.where(data[test_mask][col_score1]>=0.5,1,0)), \n",
    "              recall_score(data[test_mask][col_target],np.where(data[test_mask][col_score1]>=0.5,1,0)),\n",
    "              precision_score(data[test_mask][col_target],np.where(data[test_mask][col_score1]>=0.5,1,0)),\n",
    "              roc_auc_score(data[test_mask][col_target],np.where(data[test_mask][col_score1]>=0.5,1,0)),\n",
    "              f1_score(data[test_mask][col_target],np.where(data[test_mask][col_score1]>=0.5,1,0))],\n",
    "             index=['accuracy', 'recall', 'precision', 'roc_auc_score','f1_score'],columns=['metric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(data[test_mask][col_target],np.where(data[test_mask][col_score1]>=0.5,1,0)),\n",
    "            index=['act - 0', 'act - 1'],columns=['pred - 0', 'pred - 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result show a very good accuracy (94%) but with very poor recall (0.4% means model can only predict 4 bad from 1000 actual bad) since most of the predictor biased towards the good client, this is the direct of impact of imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate roc curves\n",
    "# fpr, tpr, th = roc_curve(data[test_mask][col_target], \n",
    "#                                  data[test_mask][col_score1])\n",
    "\n",
    "# def mcc(y_true, y_score):\n",
    "#     # True positive\n",
    "#     tp = (y_true*y_score)\n",
    "#     # False positive\n",
    "#     fp = ((y_true==0)*y_score)\n",
    "#     # True negative\n",
    "#     tn = ((y_true==0)*(y_score==0))\n",
    "#     # False negative\n",
    "#     fn = (y_true*(y_score==0))\n",
    "#     mcc = (tp*tn-fp*fn)/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "#     return mcc\n",
    "    \n",
    "\n",
    "# # calculate the g-mean for each threshold (balanced sensitivity and specificity)\n",
    "# gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# # gmeans = mcc(data[test_mask][col_target],data[test_mask][col_target])\n",
    "\n",
    "# # locate the index of the best g-mean\n",
    "# ix = argmax(gmeans)\n",
    "# print('Best Threshold=%f, G-Mean=%.3f' % (th[ix], gmeans[ix]))\n",
    "\n",
    "# # plot the roc curve for the model\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.plot([0,1], [0,1], linestyle='--')\n",
    "# plt.scatter(fpr[ix], tpr[ix],marker='o',color='k',label='Best',s=50)\n",
    "# plt.plot(fpr, tpr, marker='.', markersize=0.001,label='XGB ROC Curve',color='r')\n",
    "# # axis labels\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(data[test_mask][col_target], to_labels(data[test_mask][col_score], t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluate selected threshold to different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_predict = 'xgb_clf'\n",
    "data[col_predict] = np.where(data[col_score1] > thresholds[ix],1,0)\n",
    "pd.DataFrame([accuracy_score(data[test_mask][col_target],data[test_mask][col_predict]), \n",
    "              recall_score(data[test_mask][col_target],data[test_mask][col_predict]),\n",
    "              precision_score(data[test_mask][col_target],data[test_mask][col_predict]),\n",
    "              roc_auc_score(data[test_mask][col_target],data[test_mask][col_predict]),\n",
    "              f1_score(data[test_mask][col_target],data[test_mask][col_predict])],\n",
    "             index=['accuracy', 'recall', 'precision', 'roc_auc_score','f1_score'],columns=['metric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(data[test_mask][col_target],data[test_mask][col_predict]),\n",
    "            index=['act - 0', 'act - 1'],columns=['pred - 0', 'pred - 1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "518px",
    "width": "517px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "826px",
    "left": "89px",
    "top": "180px",
    "width": "371.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
